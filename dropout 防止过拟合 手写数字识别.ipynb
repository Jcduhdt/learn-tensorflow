{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Anaconda3\\Tensorflow\\MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting D:\\Anaconda3\\Tensorflow\\MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting D:\\Anaconda3\\Tensorflow\\MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting D:\\Anaconda3\\Tensorflow\\MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.9203,Training Accuracy 0.91236365\n",
      "Iter 1,Testing Accuracy 0.9294,Training Accuracy 0.9276364\n",
      "Iter 2,Testing Accuracy 0.9375,Training Accuracy 0.93396366\n",
      "Iter 3,Testing Accuracy 0.9391,Training Accuracy 0.94025457\n",
      "Iter 4,Testing Accuracy 0.9409,Training Accuracy 0.94403636\n",
      "Iter 5,Testing Accuracy 0.9454,Training Accuracy 0.94814545\n",
      "Iter 6,Testing Accuracy 0.9472,Training Accuracy 0.9511818\n",
      "Iter 7,Testing Accuracy 0.9495,Training Accuracy 0.95378184\n",
      "Iter 8,Testing Accuracy 0.9509,Training Accuracy 0.9561091\n",
      "Iter 9,Testing Accuracy 0.9529,Training Accuracy 0.95796365\n",
      "Iter 10,Testing Accuracy 0.9542,Training Accuracy 0.95965457\n",
      "Iter 11,Testing Accuracy 0.9558,Training Accuracy 0.9616\n",
      "Iter 12,Testing Accuracy 0.9577,Training Accuracy 0.9622909\n",
      "Iter 13,Testing Accuracy 0.958,Training Accuracy 0.96296364\n",
      "Iter 14,Testing Accuracy 0.96,Training Accuracy 0.9653636\n",
      "Iter 15,Testing Accuracy 0.9601,Training Accuracy 0.96583635\n",
      "Iter 16,Testing Accuracy 0.9619,Training Accuracy 0.96770906\n",
      "Iter 17,Testing Accuracy 0.9634,Training Accuracy 0.9679818\n",
      "Iter 18,Testing Accuracy 0.9641,Training Accuracy 0.9689818\n",
      "Iter 19,Testing Accuracy 0.9638,Training Accuracy 0.9698\n",
      "Iter 20,Testing Accuracy 0.9665,Training Accuracy 0.9707091\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"D:\\Anaconda3\\Tensorflow\\MNIST_data\",one_hot=True)\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#创建一个神经网络784-2000-2000-1000-10\n",
    "#tf.truncated_normal(shape, mean, stddev) :shape表示生成张量的维度，mean是均值，stddev是标准差。\n",
    "#这个函数产生正太分布，均值和标准差自己设定。\n",
    "#这是一个截断的产生正态分布的函数，就是说产生正态分布的值如果与均值的差值大于两倍的标准差，那就重新生成。\n",
    "w1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "L1 =tf.nn.tanh(tf.matmul(x,w1)+b1)\n",
    "#tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None,name=None)\n",
    "#x输入，keep_prob每个神经元的使用概率\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([2000,2000],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([2000])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,w2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([1000])+0.1)\n",
    "L3 = tf.nn.tanh(tf.matmul(L2_drop,w3)+b3)\n",
    "L3_drop = tf.nn.dropout(L3,keep_prob)\n",
    "\n",
    "w4 = tf.Variable(tf.truncated_normal([1000,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "#通过Softmax回归，将logistic的预测二分类的概率的问题推广到了n分类的概率的问题\n",
    "prediction = tf.nn.softmax(tf.matmul(L3_drop,w4)+b4)\n",
    "\n",
    "#代价函数\n",
    "#loss = tf.reduce_mean(tf.square(prediction-y))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(test_acc) + \",Training Accuracy \" + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
