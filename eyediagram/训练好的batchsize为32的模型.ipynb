{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用训练好的模型进行测试\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess,path)\n",
    "# print(sess.run(loss,feed_dict={x: batch_img,y: batch_labels,keep_prob:1.0}))\n",
    "# 关键就是这几句，因为第一句需要保存变量，所以需要在代码内添加与训练时定义相同的变量\n",
    "# 因为我实在不知道该怎么简化需要的代码，就把训练时定义的全部拷过来了\n",
    "# 然后path就是存放训练好的模型的路径\n",
    "# print哪里与训练时喂的东西要一致\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 是用matlab来产生的h5文件，data字段存的是眼图数据 labels字段存的是误码率\n",
    "def read_h5(path):\n",
    "    f = h5py.File(path)\n",
    "    img = np.transpose(f['data'])\n",
    "    img = np.float32(img)\n",
    "    labels = np.float32(f['labels'])\n",
    "    return img,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 跟着网上学的next_batch喂给placeholder\n",
    "def next_batch(data, labels, batch_size):  \n",
    "    index = [ i for i in range(0,len(labels)) ]  \n",
    "    #np.random.shuffle(index)  # 打乱顺序\n",
    "    batch_data = []\n",
    "    batch_labels = [] \n",
    "    for i in range(0,batch_size):  \n",
    "        data_app = np.reshape(data[index[i],:,:],[-1,360*460])# 这里使用np.reshape,而不使用tf.reshape因为使用了后者报错了，似乎是格式不对\n",
    "        # 就是这个错  setting an array element with a sequence.\n",
    "        batch_data.append(data_app[0])# 有[0]是因为data_app的结果是[1，165600],这样喂给placeholder时就是[batch_size,1,165600]\n",
    "        # 而不是想要的[batch_size,165600]\n",
    "        batch_labels.append(-np.log2(labels[index[i]]+1e-5))# 因为误码率是在0-0.3左右，区分度不高，所以就采用-log2(ber+1e-5)\n",
    "        # 负号是为了让标签为正值,以2为底是感觉好随便取的，添加1e-5是为了避免误码率为0时的情况\n",
    "    return batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from F:/code/eyediagram/my_eye/outcome/715/net/eyesight_net.ckpt\n",
      "0.21540618\n",
      "[[ 7.9948163]\n",
      " [10.243273 ]\n",
      " [ 2.4962144]\n",
      " [ 9.4180765]\n",
      " [ 4.4664   ]\n",
      " [ 2.3092818]\n",
      " [ 2.3420534]\n",
      " [ 3.9479337]\n",
      " [10.477301 ]\n",
      " [10.680367 ]\n",
      " [ 2.3195014]\n",
      " [10.905551 ]\n",
      " [10.5135565]\n",
      " [ 3.9298534]\n",
      " [ 7.6457696]\n",
      " [ 2.002858 ]\n",
      " [ 3.4149704]\n",
      " [ 9.541807 ]\n",
      " [ 7.394179 ]\n",
      " [10.3097315]\n",
      " [ 5.6459136]\n",
      " [ 2.1599026]\n",
      " [ 8.209622 ]\n",
      " [ 9.68728  ]\n",
      " [ 5.270032 ]\n",
      " [ 5.501942 ]\n",
      " [ 5.8692575]\n",
      " [ 3.10554  ]\n",
      " [ 5.279142 ]\n",
      " [ 2.4914703]\n",
      " [ 5.288733 ]\n",
      " [ 2.3520343]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_path = 'F:/code/eyediagram/shibai/eyediagram_sur.h5'\n",
    "sur_img,sur_labels = read_h5(test_path)\n",
    "\n",
    "\n",
    "#n_batch好像没用到啊\n",
    "# n_batch = train_img.shape[0] // batch_size  # 201/10\n",
    "\n",
    "#收集参数的均值方差\n",
    "def Variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.squre(var-mean)))#标准差公式\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "#初始化权值        \n",
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布\n",
    "    return tf.Variable(initial,name)\n",
    "\n",
    "# 初始化偏置值\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial,name)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape `[batch批次大小, in_height长, in_width宽, in_channels通道数（黑白为1，彩色为3]`\n",
    "    #W filter / kernel 滤波器卷积核tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A `string` from: `\"SAME在外补0\", \"VALID不补0\"`\n",
    "    #2d表示二维卷积操作\n",
    "    return tf.nn.conv2d(x,W,[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pooling_2x2(x):\n",
    "    #ksize [1,x,y,1]窗口大小\n",
    "    #ksize[0,3]默认为1，[1，2]为窗口大小\n",
    "    # strides 步长\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,165600],name='x_input')# 360*460\n",
    "    y = tf.placeholder(tf.float32,[None,1],name = 'y_input')#ber\n",
    "    with tf.name_scope('x_image'):\n",
    "        #向量转换成图片形状\n",
    "        #改变x的格式转为4D的向量[batch, in_height, in_width, in_channels]`\n",
    "        #-1就是batch_size的大小\n",
    "        x_image = tf.reshape(x,[-1,360,460,1],name='x_image')\n",
    "        \n",
    "#第一个卷积层        \n",
    "with tf.name_scope('conv1'):\n",
    "    with tf.name_scope('conv1_w'):\n",
    "        #5*5的采样窗口，1/32表示输入/输出通道数\n",
    "        conv1_w = weight_variable([5,5,1,16],name='conv1_w') # 32个卷积核\n",
    "    with tf.name_scope('conv1_b'):\n",
    "        #一个卷积核一个偏置\n",
    "        conv1_b = bias_variable([16],name='conv1_b')\n",
    "    #把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数    \n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 = conv2d(x_image,conv1_w) + conv1_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_1 = tf.nn.tanh(conv2d_1)\n",
    "    with tf.name_scope('conv1_pool'):\n",
    "        conv1_pool = max_pooling_2x2(h_conv_1)\n",
    "        \n",
    "#第二个卷积层        \n",
    "with tf.name_scope('conv2'):\n",
    "    with tf.name_scope('conv2_w'):\n",
    "        conv2_w = weight_variable([5,5,16,32],name='conv2_w')\n",
    "    with tf.name_scope('conv2_b'):\n",
    "        conv2_b = bias_variable([32],name='conv2_b')\n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 = conv2d(conv1_pool,conv2_w) + conv2_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_2 = tf.nn.tanh(conv2d_2)\n",
    "    with tf.name_scope('conv2_pool'):\n",
    "        conv2_pool = max_pooling_2x2(h_conv_2)\n",
    "        \n",
    "#第三个卷积层        \n",
    "with tf.name_scope('conv3'):\n",
    "    with tf.name_scope('conv3_w'):\n",
    "        conv3_w = weight_variable([5,5,32,64],name='conv3_w')\n",
    "    with tf.name_scope('conv3_b'):\n",
    "        conv3_b = bias_variable([64],name='conv3_b')\n",
    "    with tf.name_scope('conv2d_3'):\n",
    "        conv2d_3 = conv2d(conv2_pool,conv3_w) + conv3_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_3 = tf.nn.tanh(conv2d_3)\n",
    "    with tf.name_scope('conv3_pool'):\n",
    "        conv3_pool = max_pooling_2x2(h_conv_3)\n",
    "        \n",
    "#第四个卷积层        \n",
    "with tf.name_scope('conv4'):\n",
    "    with tf.name_scope('conv4_w'):\n",
    "        conv4_w = weight_variable([3,3,64,128],name='conv4_w')\n",
    "    with tf.name_scope('conv4_b'):\n",
    "        conv4_b = bias_variable([128],name='conv4_b')\n",
    "    with tf.name_scope('conv2d_4'):\n",
    "        conv2d_4 = conv2d(conv3_pool,conv4_w) + conv4_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_4 = tf.nn.tanh(conv2d_4)\n",
    "    with tf.name_scope('conv4_pool'):\n",
    "        conv4_pool = max_pooling_2x2(h_conv_4)\n",
    "        \n",
    "#第五个卷积层        \n",
    "with tf.name_scope('conv5'):\n",
    "    with tf.name_scope('conv5_w'):\n",
    "        conv5_w = weight_variable([3,3,128,80],name='conv5_w')\n",
    "    with tf.name_scope('conv5_b'):\n",
    "        conv5_b = bias_variable([80],name='conv5_b')\n",
    "    with tf.name_scope('conv2d_5'):\n",
    "        conv2d_5 = conv2d(conv4_pool,conv5_w) + conv5_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_5 = tf.nn.tanh(conv2d_5)\n",
    "    with tf.name_scope('conv5_pool'):\n",
    "        conv5_pool = max_pooling_2x2(h_conv_5)\n",
    "        \n",
    "#第六个卷积层        \n",
    "with tf.name_scope('conv6'):\n",
    "    with tf.name_scope('conv6_w'):\n",
    "        conv6_w = weight_variable([3,3,80,40],name='conv6_w')\n",
    "    with tf.name_scope('conv6_b'):\n",
    "        conv6_b = bias_variable([40],name='conv6_b')\n",
    "    with tf.name_scope('conv2d_6'):\n",
    "        conv2d_6 = conv2d(conv5_pool,conv6_w) + conv6_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_6 = tf.nn.tanh(conv2d_6)\n",
    "    with tf.name_scope('conv6_pool'):\n",
    "        conv6_pool = max_pooling_2x2(h_conv_6)\n",
    "        \n",
    "#第七个卷积层        \n",
    "with tf.name_scope('conv7'):\n",
    "    with tf.name_scope('conv7_w'):\n",
    "        conv7_w = weight_variable([3,3,40,20],name='conv7_w')\n",
    "    with tf.name_scope('conv7_b'):\n",
    "        conv7_b = bias_variable([20],name='conv7_b')\n",
    "    with tf.name_scope('conv2d_7'):\n",
    "        conv2d_7 = conv2d(conv6_pool,conv7_w) + conv7_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_7 = tf.nn.tanh(conv2d_7)\n",
    "    with tf.name_scope('conv7_pool'):\n",
    "        conv7_pool = max_pooling_2x2(h_conv_7)\n",
    "\n",
    "#360*460的图片第一次卷积后还是360*460，第一次池化后变为180*230\n",
    "#第二次卷积后为180*230，第二次池化后变为了90*115\n",
    "# 也不知道算对没有，但是都通过运行了，应该没问题，我是到奇数时添加1再除2\n",
    "#进过上面操作后得到20张12*15的平面\n",
    "\n",
    "#第一个全连接层        \n",
    "with tf.name_scope('fc1'):\n",
    "    with tf.name_scope('fc1_w'):\n",
    "        fc1_w = weight_variable([3*4*20,60],name='fc1_w')\n",
    "    with tf.name_scope('fc1_b'):\n",
    "        fc1_b = bias_variable([60],name='fc1_b')\n",
    "    #把池化层2的输出扁平化为1维\n",
    "    with tf.name_scope('conv7_pool_flat'):\n",
    "        conv7_pool_flat = tf.reshape(conv7_pool,[-1,3*4*20],name='conv7_pool_flat')\n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(conv7_pool_flat,fc1_w) + fc1_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_fc1 = tf.nn.tanh(wx_plus_b)\n",
    "    with tf.name_scope('keep_prob_1'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    with tf.name_scope('fc1_drop'):\n",
    "        fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name='fc1_drop')\n",
    "        \n",
    "#第二个全连接层    \n",
    "with tf.name_scope('fc2'):\n",
    "    with tf.name_scope('fc2_w'):\n",
    "        fc2_w = weight_variable([60,1],name='fc2_w')\n",
    "    with tf.name_scope('fc2_b'):\n",
    "        fc2_b = bias_variable([1],name='fc2_b')\n",
    "    with tf.name_scope('wx_plus_b2'):\n",
    "        wx_plus_b2 = tf.matmul(fc1_drop,fc2_w) + fc2_b\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = wx_plus_b2\n",
    "        \n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(prediction - y),name='loss')\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_img,batch_labels = next_batch(sur_img,sur_labels,batch_size)\n",
    "    saver.restore(sess,'F:/code/eyediagram/my_eye/outcome/715/net/eyesight_net.ckpt')\n",
    "    print(sess.run(loss,feed_dict={x: batch_img,y: batch_labels,keep_prob:1.0}))\n",
    "    sur_prediction = sess.run(prediction,feed_dict = {x:batch_img,y:batch_labels,keep_prob:1.0})\n",
    "    print(sur_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.234611 ]\n",
      " [ 9.200259 ]\n",
      " [ 2.0581825]\n",
      " [ 9.802295 ]\n",
      " [ 4.821748 ]\n",
      " [ 1.9929868]\n",
      " [ 2.5516586]\n",
      " [ 4.0214043]\n",
      " [11.086088 ]\n",
      " [10.90921  ]\n",
      " [ 2.1316392]\n",
      " [11.361723 ]\n",
      " [11.150218 ]\n",
      " [ 3.5820901]\n",
      " [ 7.175022 ]\n",
      " [ 2.0993807]\n",
      " [ 3.1718984]\n",
      " [ 9.63237  ]\n",
      " [ 6.9986258]\n",
      " [11.361723 ]\n",
      " [ 5.062756 ]\n",
      " [ 1.86692  ]\n",
      " [ 8.062756 ]\n",
      " [ 9.77676  ]\n",
      " [ 5.317329 ]\n",
      " [ 6.4050794]\n",
      " [ 6.238963 ]\n",
      " [ 3.0305703]\n",
      " [ 5.039795 ]\n",
      " [ 2.1790674]\n",
      " [ 5.7673   ]\n",
      " [ 1.8534069]]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log2(sur_labels)+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00664]\n",
      " [0.0017 ]\n",
      " [0.24012]\n",
      " [0.00112]\n",
      " [0.03536]\n",
      " [0.25122]\n",
      " [0.17056]\n",
      " [0.06158]\n",
      " [0.00046]\n",
      " [0.00052]\n",
      " [0.2282 ]\n",
      " [0.00038]\n",
      " [0.00044]\n",
      " [0.0835 ]\n",
      " [0.00692]\n",
      " [0.23336]\n",
      " [0.11096]\n",
      " [0.00126]\n",
      " [0.00782]\n",
      " [0.00038]\n",
      " [0.02992]\n",
      " [0.27416]\n",
      " [0.00374]\n",
      " [0.00114]\n",
      " [0.02508]\n",
      " [0.0118 ]\n",
      " [0.01324]\n",
      " [0.12238]\n",
      " [0.0304 ]\n",
      " [0.22082]\n",
      " [0.01836]\n",
      " [0.27674]]\n"
     ]
    }
   ],
   "source": [
    "print(sur_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00391031]\n",
      " [0.00081503]\n",
      " [0.17723116]\n",
      " [0.00145176]\n",
      " [0.04522552]\n",
      " [0.20175086]\n",
      " [0.19721942]\n",
      " [0.0647868 ]\n",
      " [0.00069148]\n",
      " [0.00059938]\n",
      " [0.2003267 ]\n",
      " [0.00051132]\n",
      " [0.00067408]\n",
      " [0.06560396]\n",
      " [0.00498337]\n",
      " [0.24949525]\n",
      " [0.09374436]\n",
      " [0.00133162]\n",
      " [0.00593471]\n",
      " [0.00077788]\n",
      " [0.0199615 ]\n",
      " [0.22376138]\n",
      " [0.00336798]\n",
      " [0.00120294]\n",
      " [0.02590566]\n",
      " [0.02205736]\n",
      " [0.01709714]\n",
      " [0.11617212]\n",
      " [0.02574253]\n",
      " [0.17781495]\n",
      " [0.0255719 ]\n",
      " [0.19585964]]\n"
     ]
    }
   ],
   "source": [
    "print(2**(-sur_prediction)-1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.7296892e-03]\n",
      " [ 8.8497403e-04]\n",
      " [ 6.2888831e-02]\n",
      " [-3.3176132e-04]\n",
      " [-9.8655187e-03]\n",
      " [ 4.9469128e-02]\n",
      " [-2.6659414e-02]\n",
      " [-3.2068007e-03]\n",
      " [-2.3148471e-04]\n",
      " [-7.9381163e-05]\n",
      " [ 2.7873307e-02]\n",
      " [-1.3131715e-04]\n",
      " [-2.3407562e-04]\n",
      " [ 1.7896041e-02]\n",
      " [ 1.9366271e-03]\n",
      " [-1.6135246e-02]\n",
      " [ 1.7215639e-02]\n",
      " [-7.1620918e-05]\n",
      " [ 1.8852940e-03]\n",
      " [-3.9788269e-04]\n",
      " [ 9.9585019e-03]\n",
      " [ 5.0398618e-02]\n",
      " [ 3.7201750e-04]\n",
      " [-6.2937033e-05]\n",
      " [-8.2566217e-04]\n",
      " [-1.0257360e-02]\n",
      " [-3.8571414e-03]\n",
      " [ 6.2078834e-03]\n",
      " [ 4.6574697e-03]\n",
      " [ 4.3005049e-02]\n",
      " [-7.2118957e-03]\n",
      " [ 8.0880374e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(sur_labels-(2**(-sur_prediction)-1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009334373\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sur_labels-(2**(-sur_prediction)-1e-5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
