{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 选定使用GPU \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_h5(path):\n",
    "    f = h5py.File(path)\n",
    "    img = np.transpose(f['data'])\n",
    "    img = np.float32(img)\n",
    "    labels = np.float32(f['labels'])\n",
    "    return img,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(data, labels, batch_size):  \n",
    "    index = [ i for i in range(0,len(labels)) ]  \n",
    "    np.random.shuffle(index)  # 打乱顺序\n",
    "    batch_data = []\n",
    "    batch_labels = [] \n",
    "    for i in range(0,batch_size):  \n",
    "        data_app = np.reshape(data[index[i],:,:],[-1,360*460])# 这里使用np.reshape,而不使用tf.reshape因为使用了后者报错了，似乎是格式不对\n",
    "        # 就是这个错  setting an array element with a sequence.\n",
    "        batch_data.append(data_app[0])# 有[0]是因为data_app的结果是[1，165600],这样喂给placeholder时就是[batch_size,1,165600]\n",
    "        # 而不是想要的[batch_size,165600]\n",
    "        #batch_labels.append(1e6*labels[index[i]])\n",
    "        batch_labels.append(-np.log2(labels[index[i]]+1e-5))# 因为误码率是在0-0.3左右，区分度不高，所以就采用-log2(ber+1e-5)\n",
    "        # 负号是为了让标签为正值,以2为底是感觉好随便取的，添加1e-5是为了避免误码率为0时的情况\n",
    "    return batch_data, batch_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'conv1_w' has type str, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'conv1_w' has type str, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'conv1_w' has type str, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'conv1_w' has type str, but expected one of: int, long, bool\n",
      "Iter: 0,Testing loss: 96.04622,Training loss:78.244125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4addd80a8c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mbatch_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[1;31m#记录训练集计算的参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_img\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_path = 'F:/code/eyediagram/my_eye/eyediagram_train.h5'\n",
    "train_img,train_labels = read_h5(train_path)\n",
    "\n",
    "test_path = 'F:/code/eyediagram/my_eye/eyediagram_test.h5'\n",
    "test_img,test_labels = read_h5(test_path)\n",
    "\n",
    "\n",
    "#n_batch好像没用到啊\n",
    "# n_batch = train_img.shape[0] // batch_size  # 201/10\n",
    "\n",
    "#收集参数的均值方差\n",
    "def Variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.mean(var)\n",
    "        tf.summary.scalar('mean',mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.squre(var-mean)))#标准差公式\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "#初始化权值        \n",
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)#生成一个截断的正态分布\n",
    "    return tf.Variable(initial,name)\n",
    "\n",
    "# 初始化偏置值\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial,name)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    #x input tensor of shape `[batch批次大小, in_height长, in_width宽, in_channels通道数（黑白为1，彩色为3]`\n",
    "    #W filter / kernel 滤波器卷积核tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    #`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长\n",
    "    #padding: A `string` from: `\"SAME在外补0\", \"VALID不补0\"`\n",
    "    #2d表示二维卷积操作\n",
    "    return tf.nn.conv2d(x,W,[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pooling_2x2(x):\n",
    "    #ksize [1,x,y,1]窗口大小\n",
    "    #ksize[0,3]默认为1，[1，2]为窗口大小\n",
    "    # strides 步长\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,165600],name='x_input')# 360*460\n",
    "    y = tf.placeholder(tf.float32,[None,1],name = 'y_input')#ber\n",
    "    with tf.name_scope('x_image'):\n",
    "        #向量转换成图片形状\n",
    "        #改变x的格式转为4D的向量[batch, in_height, in_width, in_channels]`\n",
    "        #-1就是batch_size的大小\n",
    "        x_image = tf.reshape(x,[-1,360,460,1],name='x_image')\n",
    "        \n",
    "#第一个卷积层        \n",
    "with tf.name_scope('conv1'):\n",
    "    with tf.name_scope('conv1_w'):\n",
    "        #5*5的采样窗口，1/32表示输入/输出通道数\n",
    "        conv1_w = weight_variable([5,5,1,16],name='conv1_w') # 32个卷积核\n",
    "    with tf.name_scope('conv1_b'):\n",
    "        #一个卷积核一个偏置\n",
    "        conv1_b = bias_variable([16],name='conv1_b')\n",
    "    #把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数    \n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 = conv2d(x_image,conv1_w) + conv1_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_1 = tf.nn.tanh(conv2d_1)\n",
    "    with tf.name_scope('conv1_pool'):\n",
    "        conv1_pool = max_pooling_2x2(h_conv_1)\n",
    "        \n",
    "#第二个卷积层        \n",
    "with tf.name_scope('conv2'):\n",
    "    with tf.name_scope('conv2_w'):\n",
    "        conv2_w = weight_variable([5,5,16,32],name='conv2_w')\n",
    "    with tf.name_scope('conv2_b'):\n",
    "        conv2_b = bias_variable([32],name='conv2_b')\n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 = conv2d(conv1_pool,conv2_w) + conv2_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_2 = tf.nn.tanh(conv2d_2)\n",
    "    with tf.name_scope('conv2_pool'):\n",
    "        conv2_pool = max_pooling_2x2(h_conv_2)\n",
    "        \n",
    "#第三个卷积层        \n",
    "with tf.name_scope('conv3'):\n",
    "    with tf.name_scope('conv3_w'):\n",
    "        conv3_w = weight_variable([3,3,32,64],name='conv3_w')\n",
    "    with tf.name_scope('conv3_b'):\n",
    "        conv3_b = bias_variable([64],name='conv3_b')\n",
    "    with tf.name_scope('conv2d_3'):\n",
    "        conv2d_3 = conv2d(conv2_pool,conv3_w) + conv3_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_3 = tf.nn.tanh(conv2d_3)\n",
    "    with tf.name_scope('conv3_pool'):\n",
    "        conv3_pool = max_pooling_2x2(h_conv_3)\n",
    "        \n",
    "#第四个卷积层        \n",
    "with tf.name_scope('conv4'):\n",
    "    with tf.name_scope('conv4_w'):\n",
    "        conv4_w = weight_variable([3,3,64,128],name='conv4_w')\n",
    "    with tf.name_scope('conv4_b'):\n",
    "        conv4_b = bias_variable([128],name='conv4_b')\n",
    "    with tf.name_scope('conv2d_4'):\n",
    "        conv2d_4 = conv2d(conv3_pool,conv4_w) + conv4_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_4 = tf.nn.tanh(conv2d_4)\n",
    "    with tf.name_scope('conv4_pool'):\n",
    "        conv4_pool = max_pooling_2x2(h_conv_4)\n",
    "        \n",
    "#第五个卷积层        \n",
    "with tf.name_scope('conv5'):\n",
    "    with tf.name_scope('conv5_w'):\n",
    "        conv5_w = weight_variable([3,3,128,80],name='conv5_w')\n",
    "    with tf.name_scope('conv5_b'):\n",
    "        conv5_b = bias_variable([80],name='conv5_b')\n",
    "    with tf.name_scope('conv2d_5'):\n",
    "        conv2d_5 = conv2d(conv4_pool,conv5_w) + conv5_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_5 = tf.nn.tanh(conv2d_5)\n",
    "    with tf.name_scope('conv5_pool'):\n",
    "        conv5_pool = max_pooling_2x2(h_conv_5)\n",
    "        \n",
    "#第六个卷积层        \n",
    "with tf.name_scope('conv6'):\n",
    "    with tf.name_scope('conv6_w'):\n",
    "        conv6_w = weight_variable([3,3,80,40],name='conv6_w')\n",
    "    with tf.name_scope('conv6_b'):\n",
    "        conv6_b = bias_variable([40],name='conv6_b')\n",
    "    with tf.name_scope('conv2d_6'):\n",
    "        conv2d_6 = conv2d(conv5_pool,conv6_w) + conv6_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_6 = tf.nn.tanh(conv2d_6)\n",
    "    with tf.name_scope('conv6_pool'):\n",
    "        conv6_pool = max_pooling_2x2(h_conv_6)\n",
    "        \n",
    "#第七个卷积层        \n",
    "with tf.name_scope('conv7'):\n",
    "    with tf.name_scope('conv7_w'):\n",
    "        conv7_w = weight_variable([3,3,40,20],name='conv7_w')\n",
    "    with tf.name_scope('conv7_b'):\n",
    "        conv7_b = bias_variable([20],name='conv7_b')\n",
    "    with tf.name_scope('conv2d_7'):\n",
    "        conv2d_7 = conv2d(conv6_pool,conv7_w) + conv7_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_conv_7 = tf.nn.tanh(conv2d_7)\n",
    "    with tf.name_scope('conv7_pool'):\n",
    "        conv7_pool = max_pooling_2x2(h_conv_7)\n",
    "\n",
    "#360*460的图片第一次卷积后还是360*460，第一次池化后变为180*230\n",
    "#第二次卷积后为180*230，第二次池化后变为了90*115\n",
    "# 也不知道算对没有，但是都通过运行了，应该没问题，我是到奇数时添加1再除2\n",
    "#进过上面操作后得到20张12*15的平面\n",
    "\n",
    "#第一个全连接层        \n",
    "with tf.name_scope('fc1'):\n",
    "    with tf.name_scope('fc1_w'):\n",
    "        fc1_w = weight_variable([3*4*20,60],name='fc1_w')\n",
    "    with tf.name_scope('fc1_b'):\n",
    "        fc1_b = bias_variable([60],name='fc1_b')\n",
    "    #把池化层2的输出扁平化为1维\n",
    "    with tf.name_scope('conv7_pool_flat'):\n",
    "        conv7_pool_flat = tf.reshape(conv7_pool,[-1,3*4*20],name='conv7_pool_flat')\n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(conv7_pool_flat,fc1_w) + fc1_b\n",
    "    with tf.name_scope('tanh'):\n",
    "        h_fc1 = tf.nn.tanh(wx_plus_b)\n",
    "    with tf.name_scope('keep_prob_1'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    with tf.name_scope('fc1_drop'):\n",
    "        fc1_drop = tf.nn.dropout(h_fc1,keep_prob,name='fc1_drop')\n",
    "        \n",
    "#第二个全连接层    \n",
    "with tf.name_scope('fc2'):\n",
    "    with tf.name_scope('fc2_w'):\n",
    "        fc2_w = weight_variable([60,1],name='fc2_w')\n",
    "    with tf.name_scope('fc2_b'):\n",
    "        fc2_b = bias_variable([1],name='fc2_b')\n",
    "    with tf.name_scope('wx_plus_b2'):\n",
    "        wx_plus_b2 = tf.matmul(fc1_drop,fc2_w) + fc2_b\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = wx_plus_b2\n",
    "        \n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(prediction - y),name='loss')\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "    \n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #保存tensorboard\n",
    "    train_writer = tf.summary.FileWriter('F:/code/eyediagram/my_eye/try/logs/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('F:/code/eyediagram/my_eye//try/logs/test',sess.graph)\n",
    "    for i in range(1001):\n",
    "        batch_img,batch_labels = next_batch(train_img,train_labels,batch_size)\n",
    "        sess.run(train,feed_dict={x:batch_img,y:batch_labels,keep_prob:0.7})\n",
    "        #记录训练集计算的参数\n",
    "        summary = sess.run(merged,feed_dict={x:batch_img,y:batch_labels,keep_prob:1.0})\n",
    "        train_writer.add_summary(summary,i)\n",
    "        batch_test_img,batch_test_labels =  next_batch(test_img,test_labels,batch_size)\n",
    "        #记录测试集计算的参数\n",
    "        summary = sess.run(merged,feed_dict={x: batch_test_img,y: batch_test_labels,keep_prob:1.0})\n",
    "        test_writer.add_summary(summary,i)\n",
    "    \n",
    "        if i%100 == 0:\n",
    "            test_loss = sess.run(loss,feed_dict={x: batch_test_img,y: batch_test_labels,keep_prob:1.0})\n",
    "            train_loss = sess.run(loss,feed_dict={x: batch_img,y: batch_labels,keep_prob:1.0})\n",
    "            test_prediction = sess.run(prediction,feed_dict={x: batch_test_img,y: batch_test_labels,keep_prob:1.0})\n",
    "            #+str(i)是为了好连接，也可以\"Iter: %d\"  %i\n",
    "            print(\"Iter: \" + str(i)+ \",Testing loss: \" + str(test_loss) + \",Training loss:\"+str(train_loss))\n",
    "            #print(\"prediction = \" + str(test_prediction))\n",
    "    saver.save(sess,'F:/code/eyediagram/my_eye/net/my_net.ckpt')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
